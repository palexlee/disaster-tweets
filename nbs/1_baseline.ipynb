{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import feature_extraction, linear_model, preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "import spacy\n",
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "\n",
    "from nlp_helpers import generate_ngrams\n",
    "\n",
    "SEED=42\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train_clean.csv')\n",
    "df_test = pd.read_csv('../data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vector = count_vectorizer.fit_transform(df_train['text'])\n",
    "test_vector = count_vectorizer.transform(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7552, 17034)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75562219, 0.75310482, 0.75036928])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "scores = cross_val_score(clf, train_vector, df_train['target'], cv=cv, scoring='f1')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../reports/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector, val_vector, train_target, val_target = train_test_split(train_vector, df_train['target'], test_size=0.1, shuffle=True, stratify=df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = clf.fit(train_vector, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624779281930548"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.score(train_vector, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.791005291005291"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.score(val_vector, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "sample_submission['target'] = clf.predict(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('../reports/cnt_vectorized_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer with ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train_clean.csv')\n",
    "df_test = pd.read_csv('../data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    clean_tokens = [token for token in tokens if token not in STOPWORDS and token.isalnum()]\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(clean_text)\n",
    "df_test['text'] = df_test['text'].apply(clean_text)\n",
    "\n",
    "df_train.to_csv('../data/train_preprocessed.csv', index=False)\n",
    "df_test.to_csv('../data/test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>char_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, keyword, location, text, target, word_count, unique_word_count, mean_word_length, char_count, punctuation_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train.text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thetawniest_the',\n",
       " 'the_out',\n",
       " 'out_of',\n",
       " 'of_control',\n",
       " 'control_wild',\n",
       " 'wild_fires',\n",
       " 'fires_in',\n",
       " 'in_california',\n",
       " 'california_even',\n",
       " 'even_in',\n",
       " 'in_the',\n",
       " 'the_northern',\n",
       " 'northern_part',\n",
       " 'part_of',\n",
       " 'of_the',\n",
       " 'the_state',\n",
       " 'state_very',\n",
       " 'very_troubling']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = '@aria_ahrary @TheTawniest The out of control wild fires in California even in the Northern part of the state. Very troubling.'\n",
    "\n",
    "generate_ngrams(txt, 2, remove_stopwords=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['text'] = df_train['text'].map(lambda x: x + ' '.join(generate_ngrams(x, n=3)))\n",
    "#df_test['text'] = df_test['text'].map(lambda x: x + ' '.join(generate_ngrams(x, n=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7552, 110284)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def LemmaTokenizer(text):\n",
    "    return [lemma.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer(stop_words=STOPWORDS, ngram_range=(1,3))\n",
    "train_vector = count_vectorizer.fit_transform(df_train['text'])\n",
    "test_vector = count_vectorizer.transform(df_test['text'])\n",
    "train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector, val_vector, train_target, val_target = train_test_split(train_vector, df_train['target'], test_size=0.1, shuffle=True, stratify=df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.LogisticRegression()\n",
    "mdl = clf.fit(train_vector, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9883755150088287"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.score(train_vector, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148148148148148"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.score(val_vector, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../reports/sample_submission.csv')\n",
    "sample_submission['target'] = clf.predict(test_vector)\n",
    "sample_submission.to_csv('../reports/ngram_cnt_vectorized_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7552, 16895)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train_clean.csv')\n",
    "df_test = pd.read_csv('../data/test_clean.csv')\n",
    "\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer(stop_words=STOPWORDS, ngram_range=(1,1))\n",
    "train_vector = vectorizer.fit_transform(df_train['text'])\n",
    "test_vector = vectorizer.transform(df_test['text'])\n",
    "train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector, val_vector, train_target, val_target = train_test_split(train_vector, df_train['target'], test_size=0.1, shuffle=True, stratify=df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8897881106533255\n",
      "Test score 0.8108465608465608\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.LogisticRegression()\n",
    "mdl = clf.fit(train_vector, train_target)\n",
    "\n",
    "print('Train score {}'.format(mdl.score(train_vector, train_target)))\n",
    "print('Test score {}'.format(mdl.score(val_vector, val_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../reports/sample_submission.csv')\n",
    "sample_submission['target'] = clf.predict(test_vector)\n",
    "sample_submission.to_csv('../reports/tfidf_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train_clean.csv')\n",
    "df_test = pd.read_csv('../data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = list(nlp.pipe(df_train['text'], n_process=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VERB', 'DET', 'ADJ', 'NOUN', 'VERB', 'DET', 'PROPN', 'NOUN', 'NOUN', 'DET', 'NOUN', 'NOUN', 'CCONJ', 'ADJ', 'PUNCT', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'ADP', 'VERB', 'NOUN', 'ADP', 'PRON', 'PUNCT', 'PUNCT']\n",
      "['have', 'an', 'awesome', 'time', 'visit', 'the', 'CFC', 'head', 'office', 'the', 'ancop', 'site', 'and', 'ablaze', '.', 'thank', 'to', 'Tita', 'Vida', 'for', 'take', 'care', 'of', 'we', '?', '?']\n",
      "['Had', 'an', 'awesome', 'time', 'visiting', 'the', 'CFC', 'head', 'office', 'the', 'ancop', 'site', 'and', 'ablaze', '.', 'Thanks', 'to', 'Tita', 'Vida', 'for', 'taking', 'care', 'of', 'us', '?', '?']\n"
     ]
    }
   ],
   "source": [
    "idx = 42\n",
    "print([n.pos_ for n in df_train.at[idx,'text']])\n",
    "print([n.lemma_ for n in df_train.at[idx,'text']])\n",
    "print([n.text for n in df_train.at[idx,'text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f754e0f2c035ece9b0a4f998d8077dc946278dc35515802da1198acd4f82ede"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
